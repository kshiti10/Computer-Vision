{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXXCh4x6nBkb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense,Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM8QF5FyoKE_"
      },
      "source": [
        "*Step 2 : DataSet Defination*\n",
        "This is the dataset where each tuples consist if a simple English phrase and its french translation.This is a small toy dataset fro the purpose of demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmRNw5CQoJql"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    (\"hello\", \"bonjour\"),\n",
        "    (\"how are you\", \"comment ça va\"),\n",
        "    (\"thank you\", \"merci\"),\n",
        "    (\"good morning\", \"bonjour\"),\n",
        "    (\"good night\", \"bonne nuit\"),\n",
        "    (\"see you later\", \"à plus tard\"),\n",
        "    (\"I love you\", \"je t'aime\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SgiFerNqNx3"
      },
      "source": [
        "Step 3: Text Preparation\n",
        "zip(*data):Separates the data tuples into two seperate lists:one for input_text(English and onefor target_text(French)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBZARs80qklV"
      },
      "outputs": [],
      "source": [
        "input_texts,target_texts = zip(*data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EusS71qHq1rB"
      },
      "source": [
        "**Step 4 : Tokenization**\n",
        "**Tokenization():** Creates a tokenizer that will convert text into sequences of integer.\n",
        "\n",
        "**fit_on_texts():** This method creates a vocabulary from the input_texts and target_texts and assigns a unique integer to each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU2fwkVMrO9F"
      },
      "outputs": [],
      "source": [
        "input_tokenizer = Tokenizer()\n",
        "target_tokenizer = Tokenizer()\n",
        "\n",
        "input_tokenizer.fit_on_texts(input_texts)\n",
        "target_tokenizer.fit_on_texts(target_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9Wo3-Q_rije"
      },
      "source": [
        "**texts_to_sequences():** Converts each text(sentences) into a sequence of integers.Each word in the text is replaced by its corresponding integer from the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSBVMQ4XsQc4"
      },
      "outputs": [],
      "source": [
        "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
        "target_sequences = target_tokenizer.texts_to_sequences(target_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u8rkVoCsbxi"
      },
      "source": [
        "**Step 5 : Vocabulary and Sequence Length Calculations**\n",
        "\n",
        "**word_index :**\n",
        "This dictionary holds the integer mappings for each word.We add 1 to account for the 0th based indexing of sequences.input_vocab_size and\n",
        "\n",
        "**target_vocab_size:**\n",
        "Stores the size of the vocabulary for the input and target languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bE29QqqtAsK"
      },
      "outputs": [],
      "source": [
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsOlDKDUtNOg"
      },
      "source": [
        "**max_input_len and max_target_len :**\n",
        "Store the maximum length of sequences in the input and target languages,respectively.This helps with padding the sequences to a uniform length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oKC1NNZuDkS"
      },
      "outputs": [],
      "source": [
        "max_input_len = max(len(seq) for seq in input_sequences)\n",
        "max_target_len = max(len(seq) for seq in target_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpGBKCg2uI_G"
      },
      "source": [
        "**Step 6 : Padding Sequences pad_sequences() :**\n",
        "Pads each sequence to ensure that all sequences have the same length.Padding is applied to the end of the sequences(padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuaghHYnujb5"
      },
      "outputs": [],
      "source": [
        "encoder_input_data = pad_sequences(input_sequences,maxlen=max_input_len,padding=\"post\")\n",
        "decoder_input_data = pad_sequences(target_sequences,maxlen=max_target_len,padding=\"post\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
